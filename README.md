# Machine Learning Links And Lessons Learned
A summary of the resources I have come across and the lessons I have learned while studying machine learning. I've been Inspired to organize my findings by Adit Deshpande's work found on his repo [here](https://github.com/adeshpande3).

* [General](#general)
* [Hyperparameters](#hyperparameters)
* [Deep Learning](#deep-learning)
* [CNNs](#cnns)
* [RNNs](#rnnss)
* [Learning Resources](#learning-resources)
* [Datasets](*datasets)
* [Research Ideas](#research-ideas)
* [Interesting Links](#interesting-links)
* [Other](#other)

## General

* Need to update

## Hyperparameters

* Need to update

## Deep Learning

* Need to update

## CNNs

* Need to update

## RNNs

* Need to update

## Learning Resources
* [tensorflow-seq2seq-tutorials](https://github.com/ematvey/tensorflow-seq2seq-tutorials)
* [Sequence to Sequence Deep Learning (Quoc Le, Google](https://www.youtube.com/watch?v=G5RY_SUJih4)
* [Improved techniques for trainings GANs](https://arxiv.org/pdf/1606.03498.pdf)

### Hyperparameters
* [Practical recommendations for gradient-based training of deep architectures](https://arxiv.org/abs/1206.5533) by Yoshua Bengio
* [Deep Learning Book - chapter 11.4: Selecting Hyperparameters](http://www.deeplearningbook.org/contents/guidelines.html) by Ian Goodfellow, Yoshua Bengio, Aaron Courville
* [Neural Networks and Deep Learning Book - Chapter 3: How to choose a neural network's hyper-parameters?](http://neuralnetworksanddeeplearning.com/chap3.html#how_to_choose_a_neural_network's_hyper-parameters) by Michael Nielsen
* [Efficient Backprop (pdf)](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf) by Yann LeCun

### Word2Vec and Word Embeddings
* [Word2Vec Overview](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)
* [Tensorflow Word2Vec Tutorial](https://www.tensorflow.org/tutorials/word2vec)

## Datasets
* [Boston Housing Dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html)
	* Performed linear regression to predict housing prices
* [CIFAR-10 and CIFAR-100 Datasets](https://www.cs.toronto.edu/~kriz/cifar.html)
	* CIFAR-10: 60000 32x32 colour images in 10 classes (6000 images per class)
	* CIFAR-100: CIFAR-10 dataset with 100 classes and 600 images per class. 
* [Microsoft Coco](http://cocodataset.org/#home)
	* Object detection, segmentation and captioning dataset with ~330K images (>200K labeled), 1.5M object instances, 80 object categories, 91 stuff categories and 5 captions per image.
* [MNIST Handwritten Digits Dataset](http://yann.lecun.com/exdb/mnist/)
	* A popular and well understood dataset of handwritten digits used as a benchmark to test new algorithms and approaches. 
	* 28x28 images with 60,000 training and 10,000 test examples
	* Used to train my first neural network and to play around with autoencoders and denoisers
* [text8 Dataset](http://mattmahoney.net/dc/textdata.html)
	* Wikipedia article dataset 
* [Cornell Movie Dialog Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html)
	* Contains over 200,000 conversational exchanges from ~600 movies
	* [Stanford Chatbot Exercise](https://github.com/chiphuyen/tf-stanford-tutorials/tree/master/assignments/chatbot)
* [French-English Translation Corpus](http://www.statmt.org/wmt10/training-giga-fren.tar)




## Research Ideas

* Need to update

## Interesting Links

* [Google Word2Vec](https://code.google.com/archive/p/word2vec/)
* [Image-to-Image Demo](https://affinelayer.com/pixsrv/) by Christopher Hesse
* [CycleGAN](https://github.com/junyanz/CycleGAN): examples of GANs applied to transfer image styles, change primary objects etc. 
* [List of Popular GANs](https://github.com/wiseodd/generative-models)


## Other

* Need to update
